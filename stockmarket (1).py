# -*- coding: utf-8 -*-
"""StockMarket.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RD6kcDbUa5RWgn_f4qp1LlawNvfUUMTW
"""

pip install numpy pandas scikit-learn matplotlib seaborn yfinance tensorflow

import yfinance as yf

# Download the stock price data (AAPL stock for 5 years)
data = yf.download('AAPL', start='2019-01-01', end='2024-01-01')
print(data.head())

data.head()

data['Price Range'] = data['High'] - data['Low']

data['MA50'] = data['Adj Close'].rolling(window=50).mean()

data['MA200'] = data['Adj Close'].rolling(window=200).mean()

data.head()

data = data.dropna(subset=['MA50', 'MA200'])



data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = data.corr()

plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(data.index, data['Adj Close'], label='Adjusted Close Price')
plt.title('Adjusted Close Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data['Price Range'], bins=50, kde=True)
plt.title('Distribution of Price Range')
plt.xlabel('Price Range')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(data.index, data['Price Range'], label='Price Range')
plt.title('Price Range (Volatility) Over Time')
plt.xlabel('Date')
plt.ylabel('Price Range')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(data.index, data['Adj Close'], label='Adjusted Close', color='b')
plt.twinx()
plt.plot(data.index, data['Volume'], label='Volume', color='g', alpha=0.3)
plt.title('Volume vs Adjusted Close Price')
plt.xlabel('Date')
plt.legend(loc='upper left')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=data['Price Range'])
plt.title('Boxplot of Price Range')
plt.show()

data.head()

# Clipping extreme values to the 95th percentile
data['Price Range'] = data['Price Range'].clip(lower=data['Price Range'].quantile(0.05),
                                               upper=data['Price Range'].quantile(0.95))

plt.figure(figsize=(10, 6))
sns.boxplot(x=data['Price Range'])
plt.title('Boxplot of Price Range')
plt.show()

data.corr()

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Initialize the MinMaxScaler for features
scaler = MinMaxScaler(feature_range=(0, 1))

# Scale the features (X), here ['Price Range', 'MA50', 'MA200']
X_scaled = scaler.fit_transform(data[['Price Range', 'MA50', 'MA200']])

# Convert scaled features to a DataFrame for clarity (optional)
X_scaled = pd.DataFrame(X_scaled, columns=['Price Range', 'MA50', 'MA200'])

# Initialize a new MinMaxScaler for the target variable
y_scaler = MinMaxScaler(feature_range=(0, 1))

# Scale the target variable (y), here 'Adj Close'
y_scaled = y_scaler.fit_transform(data[['Adj Close']])









from sklearn.model_selection import train_test_split

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)

import numpy as np

# Reshape the data to 3D for LSTM [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Initialize a simpler LSTM model
model = Sequential()

# First LSTM layer with fewer units and Dropout regularization
model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2))

# Second LSTM layer with fewer units
model.add(LSTM(units=32, return_sequences=False))
model.add(Dropout(0.2))

# Fully connected Dense layer
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(1))  # Predicting a single value (Adj Close)

# Compile the model with a reduced learning rate
model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

# Model summary
model.summary()

"""Sequential(): Initializes a sequential model, which means layers are added one after the other.
LSTM(units=50): Adds an LSTM layer with 50 units (neurons). This is the memory unit that will capture patterns in the time series. The return_sequences=True allows the LSTM layer to return the entire sequence of output for the next LSTM layer.
Dropout(0.2): This adds dropout regularization to prevent overfitting. 0.2 means that 20% of the neurons are randomly dropped during training.
Dense(1): This adds a fully connected output layer with 1 unit, as we are predicting the adjusted close price (a single value).
model.compile(): Compiles the model with the Adam optimizer and the Mean Squared Error (MSE) loss function, which is commonly used for regression tasks.

"""

# from tensorflow.keras.callbacks import EarlyStopping
# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_split=0.2)

# Make predictions on the test set
predictions = model.predict(X_test)

# Inverse transform the predictions back to the original scale
predictions = y_scaler.inverse_transform(predictions)

import matplotlib.pyplot as plt

# Plot the predicted and actual values
plt.figure(figsize=(10, 6))
plt.plot(data.index[len(data) - len(y_test):], y_scaler.inverse_transform(y_test), label='Actual Price')
plt.plot(data.index[len(data) - len(predictions):], predictions, label='Predicted Price')
plt.title('Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(y_test), predictions))
print('Root Mean Squared Error:', rmse)

percentage_error = (rmse / y_test.mean()) * 100
print(f"Percentage RMSE: {percentage_error}%")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Initialize the GRU model
model = Sequential()

# First GRU layer with Dropout
model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2))

# Second GRU layer with Dropout
model.add(GRU(units=32, return_sequences=False))
model.add(Dropout(0.2))

# Fully connected Dense layer
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(1))  # Predicting a single value (Adj Close)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')

model.summary()

history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_split=0.2)

# Make predictions on the test set
predictions = model.predict(X_test)

# Inverse transform the predictions back to the original scale
predictions = y_scaler.inverse_transform(predictions)

import matplotlib.pyplot as plt

# Plot the predicted and actual values
plt.figure(figsize=(10, 6))
plt.plot(data.index[len(data) - len(y_test):], y_scaler.inverse_transform(y_test), label='Actual Price')
plt.plot(data.index[len(data) - len(predictions):], predictions, label='Predicted Price')
plt.title('Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(y_test), predictions))
print('Root Mean Squared Error:', rmse)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input

# Initialize the 1D CNN model
model = Sequential()

# Add Input Layer explicitly
model.add(Input(shape=(X_train.shape[1], 1)))  # Input shape: (sequence_length, num_features)

# First 1D Convolutional layer with padding='same' to maintain input size
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))

# Second 1D Convolutional layer
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))

# Flatten the output from the convolutional layers
model.add(Flatten())

# Fully connected Dense layer
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(1))  # Predicting a single value (Adj Close)

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Model summary
model.summary()

history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_split=0.2)

# Make predictions on the test set
predictions = model.predict(X_test)

# Inverse transform the predictions back to the original scale
predictions = y_scaler.inverse_transform(predictions)

import matplotlib.pyplot as plt

# Plot the predicted and actual values
plt.figure(figsize=(10, 6))
plt.plot(data.index[len(data) - len(y_test):], y_scaler.inverse_transform(y_test), label='Actual Price')
plt.plot(data.index[len(data) - len(predictions):], predictions, label='Predicted Price')
plt.title('Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(y_test), predictions))
print('Root Mean Squared Error:', rmse)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Initialize the Dense model
model = Sequential()

# Adding Dense layers
model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(units=32, activation='relu'))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(1))  # Predicting a single value (Adj Close)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')

model.summary()

history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_split=0.2)

# Make predictions on the test set
predictions = model.predict(X_test)

# Inverse transform the predictions back to the original scale
predictions = y_scaler.inverse_transform(predictions)

import matplotlib.pyplot as plt

# Plot the predicted and actual values
plt.figure(figsize=(10, 6))
plt.plot(data.index[len(data) - len(y_test):], y_scaler.inverse_transform(y_test), label='Actual Price')
plt.plot(data.index[len(data) - len(predictions):], predictions, label='Predicted Price')
plt.title('Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(y_test), predictions))
print('Root Mean Squared Error:', rmse)

